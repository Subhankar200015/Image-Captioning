{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8706341,"sourceType":"datasetVersion","datasetId":5222333}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# # This Python 3 environment comes with many helpful analytics libraries installed\n# # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# # For example, here's several helpful packages to load\n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# # Input data files are available in the read-only \"../input/\" directory\n# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torchvision.models as models\nimport torch.nn as nn\nimport torch","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Encoder**","metadata":{}},{"cell_type":"code","source":"import torchvision.models as models\nimport torch.nn as nn\nimport torch\n\nclass EncoderCNN(nn.Module):\n    def __init__(self, encoded_image_size=14):\n        super(EncoderCNN, self).__init__()\n        resnet = models.resnet50(pretrained=True)\n        modules = list(resnet.children())[:-1]  # Remove final FC layer\n        self.resnet = nn.Sequential(*modules)\n        self.embed = nn.Linear(resnet.fc.in_features, 256)  # Project features to embedding size\n\n    def forward(self, images):\n        with torch.no_grad():\n            features = self.resnet(images)  # (batch_size, 2048, 1, 1)\n        features = features.view(features.size(0), -1)  # (batch_size, 2048)\n        features = self.embed(features)  # (batch_size, 256)\n        return features","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Decoder** ","metadata":{}},{"cell_type":"code","source":"class DecoderRNN(nn.Module):\n    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1):\n        super(DecoderRNN, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_size)\n        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n        self.linear = nn.Linear(hidden_size, vocab_size)\n\n    def forward(self, features, captions):\n        embeddings = self.embedding(captions[:, :-1])  # skip <end> token\n        inputs = torch.cat((features.unsqueeze(1), embeddings), 1)  # prepend image features\n        hiddens, _ = self.lstm(inputs)\n        outputs = self.linear(hiddens)\n        return outputs","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.nn.utils.rnn import pad_sequence\n\ndef collate_fn(batch):\n    images, captions = zip(*batch)\n    images = torch.stack(images)\n    captions = pad_sequence(captions, batch_first=True, padding_value=vocab['<pad>'])\n    return images, captions","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from collections import Counter\n\nclass Vocabulary:\n    def __init__(self, freq_threshold):\n        self.freq_threshold = freq_threshold\n        self.itos = {0: \"<pad>\", 1: \"<start>\", 2: \"<end>\", 3: \"<unk>\"} # index-to-string mapping (used to decode).\n        self.stoi = {v: k for k, v in self.itos.items()} # string-to-index mapping (used to encode).\n\n    def build_vocabulary(self, sentence_list):\n        frequencies = Counter()\n        idx = 4\n\n        for sentence in sentence_list:\n            for word in sentence.lower().split():\n                frequencies[word] += 1\n                if frequencies[word] == self.freq_threshold:\n                    self.stoi[word] = idx\n                    self.itos[idx] = word\n                    idx += 1\n\n    def __len__(self):\n        return len(self.itos)\n    \n    def __getitem__(self, token):\n        return self.stoi.get(token, self.stoi[\"<unk>\"])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torchvision import transforms\n\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                         std=[0.229, 0.224, 0.225]),\n])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom PIL import Image\nfrom torch.utils.data import Dataset\n\nclass CustomCaptionDataset(Dataset):\n    def __init__(self, csv_file, image_folder, vocab, transform=None):\n        self.df = pd.read_csv(csv_file)\n        self.image_folder = image_folder\n        self.vocab = vocab\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        img_name = self.df.iloc[idx]['filename']\n        caption = self.df.iloc[idx]['caption']\n\n        # print(img_name + \",\" + caption)\n        \n        # Load and transform image\n        image = Image.open(f\"{self.image_folder}/{img_name}\").convert('RGB')\n        if self.transform:\n            image = self.transform(image)\n\n        # Tokenize and numericalize caption\n        tokens = [self.vocab['<start>']] + [self.vocab.get(word, self.vocab['<unk>']) for word in caption.lower().split()] + [self.vocab['<end>']]\n        caption_tensor = torch.tensor(tokens)\n\n        return image, caption_tensor\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\n# Create vocab\nvocab_builder = Vocabulary(freq_threshold=2)\ncaptions_df = pd.read_csv(\"/kaggle/input/caption-data/custom_captions_dataset/train.csv\")\nvocab_builder.build_vocabulary(captions_df['caption'].tolist())\n\n# Assign\nvocab = vocab_builder.stoi\n\n# Load dataset\ntrain_dataset = CustomCaptionDataset(\n    csv_file=\"/kaggle/input/caption-data/custom_captions_dataset/train.csv\",\n    image_folder=\"/kaggle/input/caption-data/custom_captions_dataset/train\",\n    vocab=vocab,\n    transform=transform\n)\n\n# print(type(train_dataset))\n# print(train_dataset[0])\n\ndata_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)\n# print(data_loader)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Training Loop**","metadata":{}},{"cell_type":"code","source":"import torch.optim as optim\nimport torch.nn as nn\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nencoder = EncoderCNN().to(device)\ndecoder = DecoderRNN(embed_size=256, hidden_size=512, vocab_size=len(vocab)).to(device)\n\ncriterion = nn.CrossEntropyLoss(ignore_index=vocab['<pad>'])\noptimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=3e-4)\n\nnum_epochs = 1\n\nfor epoch in range(num_epochs):\n    for imgs, caps in data_loader:\n        imgs, caps = imgs.to(device), caps.to(device)\n\n        features = encoder(imgs)\n        outputs = decoder(features, caps)\n\n        target = caps[:, 1:]               # remove <start> token, shape = [B, T-1]\n\n        # Trim output to match target length\n        outputs = outputs[:, :target.size(1), :]  # ensure same sequence length\n\n        loss = criterion(outputs.reshape(-1, outputs.shape[2]), target.reshape(-1))\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    print(f\"Epoch {epoch + 1}, Loss: {loss.item():.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Saving the Model**","metadata":{}},{"cell_type":"code","source":"torch.save({\n    'encoder_state_dict': encoder.state_dict(),\n    'decoder_state_dict': decoder.state_dict(),\n    'vocab': vocab_builder,  # saving full vocab object, not just stoi\n}, 'caption_model.pth')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Caption Generation**","metadata":{}},{"cell_type":"code","source":"import os\n\n# Recreate the model architectures\nencoder = EncoderCNN().to(device)\ndecoder = DecoderRNN(embed_size=256, hidden_size=512, vocab_size=len(vocab_builder)).to(device)\n\n# Load checkpoint\ncheckpoint = torch.load('caption_model.pth', map_location=device)\n\nencoder.load_state_dict(checkpoint['encoder_state_dict'])\ndecoder.load_state_dict(checkpoint['decoder_state_dict'])\nvocab_builder = checkpoint['vocab']\nvocab = vocab_builder.itos  # or .stoi depending on usage\n\n\ndef generate_caption(encoder, decoder, image_path, idx2word, transform, device, max_len=20):\n    image = Image.open(image_path).convert('RGB')\n    image = transform(image).unsqueeze(0).to(device)\n    \n    encoder.eval()\n    decoder.eval()\n    \n    with torch.no_grad():\n        features = encoder(image)\n        inputs = features.unsqueeze(1)\n        states = None\n        sampled_ids = []\n        \n        for _ in range(max_len):\n            hiddens, states = decoder.lstm(inputs, states)\n            outputs = decoder.linear(hiddens.squeeze(1))\n            predicted = outputs.argmax(1)\n            sampled_ids.append(predicted.item())\n            inputs = decoder.embedding(predicted).unsqueeze(1)\n    \n    caption = []\n    for word_id in sampled_ids:\n        word = idx2word.get(word_id, \"<unk>\")\n        if word == '<end>':\n            break\n        caption.append(word)\n    \n    return ' '.join(caption)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_dir = '/kaggle/input/caption-data/custom_captions_dataset/test'\n\nfor img_name in os.listdir(test_dir):\n    img_path = os.path.join(test_dir, img_name)\n    caption = generate_caption(encoder, decoder, img_path, vocab_builder.itos, transform, device)\n    print(f\"{img_name}: {caption}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# /kaggle/input/caption-data/custom_captions_dataset/val.csv\n# /kaggle/input/caption-data/custom_captions_dataset/train.csv\n# /kaggle/input/caption-data/custom_captions_dataset/test.csv","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}